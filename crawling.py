import re, time, json, requests
from urllib.parse import urlparse, parse_qs, urlunparse, urlencode
from datetime import datetime, date
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from webdriver_manager.chrome import ChromeDriverManager
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ===== ì„¤ì • =====
ACCESS_TOKEN = "eyJhbGciOiJIUzI1NiJ9.eyJpZCI6MTMsIm5hbWUiOiLsi5zsiqTthZwiLCJ1c2VybmFtZSI6InN5c3RlbSIsInJvbGUiOlsiUk9MRV9TWVNURU0iXSwiaWF0IjoxNzU3OTI1MDk4LCJleHAiOjE3NTc5MjY4OTh9.DOiOf_BEIPqAg0x_peWMd2aCvVdIUTe4pA6gxv1uIEk"  # ğŸ” êµì²´
CUTOFF_DATE_STR = "2025-08-25"
CUTOFF_DATE: date = datetime.strptime(CUTOFF_DATE_STR, "%Y-%m-%d").date()

LIST_BASE = "https://www.kmou.ac.kr/kmou/na/ntt/selectNttList.do?mi=2032&bbsId=10373"
POST_ENDPOINT = "http://58.238.182.100:9000/api/system/crawling-notice/univ"

TIMEOUT_SEC = 12
INCLUDE_STICKY_ONLY_FIRST_PAGE = True

# ì „ì†¡ ë°°ì¹˜/ì¬ì‹œë„ (ê³ ì • ë°°ì¹˜ 10)
BATCH_SIZE = 10
CONNECT_TIMEOUT = 10
READ_TIMEOUT = 180
MAX_RETRIES_PER_BATCH = 3

# ===== ë“œë¼ì´ë²„ =====
options = webdriver.ChromeOptions()
options.add_argument("--headless=new")
options.add_argument("--disable-gpu")
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
options.add_argument("--ignore-certificate-errors")
options.add_argument("--lang=ko-KR")
options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
wait = WebDriverWait(driver, TIMEOUT_SEC)

ABS_BASE = "https://www.kmou.ac.kr"

# ===== ìœ í‹¸ =====
def normalize_url(u: str) -> str:
    """ì¿¼ë¦¬ ì •ë ¬/ì¤‘ë³µ ì œê±°, fragment ì œê±° â†’ URL ì •ê·œí™”"""
    try:
        p = urlparse(u)
        q = parse_qs(p.query, keep_blank_values=True)
        items = []
        for k in sorted(q.keys()):
            for v in sorted(q[k]):
                items.append((k, v))
        new_q = urlencode(items)
        return urlunparse((p.scheme, p.netloc, p.path, "", new_q, ""))
    except Exception:
        return (u or "").strip()

def absolutize_url(v: str) -> str:
    v = (v or "").strip()
    return ABS_BASE + v if v.startswith("/") else v

def parse_row_date(text: str):
    m = re.search(r"\d{4}[.\-]\d{2}[.\-]\d{2}", text or "")
    if not m: return None
    s = m.group().replace(".", "-")
    return datetime.strptime(s, "%Y-%m-%d").date()

def extract_nttSn_from_any(s: str):
    # URL ì¿¼ë¦¬
    try:
        q = parse_qs(urlparse(s).query)
        if q.get("nttSn"): return q["nttSn"][0]
    except Exception:
        pass
    # onclick íŒ¨í„´
    for p in (r"selectNttInfo\(['\"]?(\d+)['\"]?\)",
              r"fn_egov_select_nttInfo\(['\"]?(\d+)['\"]?\)",
              r"viewNtt\(['\"]?(\d+)['\"]?\)",
              r"nttSn\s*=\s*['\"]?(\d+)['\"]?"):
        m = re.search(p, s or "")
        if m: return m.group(1)
    return None

def build_detail_url(href: str, onclick: str):
    href = (href or "").strip()
    if href and not href.startswith("#") and not href.lower().startswith("javascript"):
        return absolutize_url(href)
    ntt = extract_nttSn_from_any(onclick or "")
    if ntt:
        return f"{ABS_BASE}/kmou/na/ntt/selectNttInfo.do?nttSn={ntt}&mi=2032"
    return None

def fix_relative_urls_in_soup(soup: BeautifulSoup):
    for tag in soup.find_all(["a", "img"]):
        attr = "href" if tag.name == "a" else "src"
        if not tag.has_attr(attr): continue
        tag[attr] = absolutize_url(tag.get(attr) or "")

def parse_detail_date_from_html(content_html: str):
    try:
        s = BeautifulSoup(content_html, "html.parser")
        lab = s.find(lambda t: t.name in ("th","dt") and "ë“±ë¡ì¼" in t.get_text(strip=True))
        if lab:
            nxt = lab.find_next_sibling()
            if nxt:
                d = parse_row_date(nxt.get_text(" ", strip=True))
                if d: return d
        return parse_row_date(s.get_text(" ", strip=True))
    except Exception:
        return None

def coalesce_date(list_row_date, content_html) -> str:
    if list_row_date: return list_row_date.isoformat()
    d2 = parse_detail_date_from_html(content_html)
    return (d2 or date.today()).isoformat()

def pick_thumbnail_from_content(html: str):
    """
    ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì˜ 'ì‹¤ì œ' ì´ë¯¸ì§€ ì¤‘ ì²« ë²ˆì§¸ë§Œ ì¸ë„¬ë¡œ.
    ë¡œê³ /ì•„ì´ì½˜/ìŠ¤í”„ë¼ì´íŠ¸/íˆ¬ëª…gif/svg ë“±ì€ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì œì™¸.
    """
    s = BeautifulSoup(html, "html.parser")
    for img in s.select("img"):
        src = (img.get("src") or img.get("data-src") or "").strip()
        if not src: continue
        src = absolutize_url(src)
        low = src.lower()
        if any(x in low for x in ["blank.gif", "spacer", "icon", "ico_", "/ico/", "sprite", "emoticon", "emoji", "logo"]):
            continue
        if low.endswith(".svg"):
            continue
        return src
    return None

def wait_for_list():
    """ëª©ë¡ í…Œì´ë¸” ë“±ì¥ê¹Œì§€ ëŒ€ê¸°(ì—¬ëŸ¬ ì…€ë ‰í„° OR)"""
    wait.until(EC.any_of(
        EC.presence_of_element_located((By.CSS_SELECTOR, ".BD_list tbody tr")),
        EC.presence_of_element_located((By.CSS_SELECTOR, ".board_list tbody tr")),
        EC.presence_of_element_located((By.CSS_SELECTOR, ".bd-list tbody tr")),
        EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr"))
    ))

# ===== ëª©ë¡ íŒŒì‹± =====
def scan_list_page():
    soup = BeautifulSoup(driver.page_source, "html.parser")
    rows = soup.select("tbody tr")
    items = []
    for tr in rows:
        a = (tr.select_one("a[href*='selectNttInfo']") or
             tr.select_one("a[onclick*='nttSn']") or
             tr.select_one("a"))
        if not a:
            continue

        title = (a.get_text(strip=True) or a.get("title") or "").strip()
        detail_url = build_detail_url(a.get("href",""), a.get("onclick",""))
        if not detail_url:
            continue

        row_txt_all = tr.get_text(" ", strip=True)
        sticky = any(k in row_txt_all for k in ("ê³µì§€", "Notice", "NOTICE", "TOP", "Top"))

        row_date = None
        for td in tr.find_all("td"):
            row_date = parse_row_date(td.get_text(" ", strip=True))
            if row_date:
                break

        ntt_sn = extract_nttSn_from_any(detail_url) or extract_nttSn_from_any(a.get("onclick",""))
        items.append({
            "title": title,
            "detail_url": detail_url,
            "row_date": row_date,
            "is_sticky": sticky,
            "ntt_sn": ntt_sn
        })
    return items

# ===== ìƒì„¸ íŒŒì‹± =====
def get_detail(detail_url: str):
    driver.get(detail_url)
    html = ""
    for sel in (".BD_table", ".bd_view", ".board_view", ".bd-view", ".BD_view", ".view"):
        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, sel)))
            elem = driver.find_element(By.CSS_SELECTOR, sel)
            html = elem.get_attribute("outerHTML") or ""
            if html.strip():
                break
        except Exception:
            continue
    if not html:
        raise RuntimeError("ìƒì„¸ ì»¨í…Œì´ë„ˆ ë¯¸ë°œê²¬")

    soup = BeautifulSoup(html, "html.parser")

    # <caption> ì œê±°
    cap = soup.find("caption")
    if cap:
        cap.decompose()

    # ì œëª© í–‰(<tr> ì•ˆì— <th class="title" colspan="4">) ì œê±°
    for tr in soup.find_all("tr"):
        th = tr.find("th", class_="title")
        if th and th.has_attr("colspan") and th["colspan"] == "4":
            tr.decompose()

    # data-src â†’ src ì¹˜í™˜
    for img in soup.find_all("img"):
        if img.has_attr("data-src") and not img.get("src"):
            img["src"] = img["data-src"]

    # ìƒëŒ€ê²½ë¡œ ë³´ì •
    fix_relative_urls_in_soup(soup)

    # HTML ë³€í™˜
    content_html = str(soup)

    # ë³¸ë¬¸ ë‚´ë¶€ ì²« ìœ íš¨ ì´ë¯¸ì§€ë§Œ ì¸ë„¬
    thumbnail = pick_thumbnail_from_content(content_html)
    return content_html, thumbnail

def fetch_detail_with_retry(url, retries=2):
    last_err = None
    for _ in range(retries + 1):
        try:
            return get_detail(url)
        except Exception as e:
            last_err = e
    raise last_err

# ===== payload í—¬í¼ =====
def make_payload(it, content_html, thumb, is_sticky):
    return {
        "title": it["title"],
        "content": content_html,
        "url": normalize_url(it["detail_url"]),
        "date": coalesce_date(it["row_date"], content_html),
        "sticky": is_sticky,
        "img": [thumb] if thumb else []   # âœ… í•­ìƒ ë°°ì—´(ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´)
    }

# ===== ì „ì†¡(ê³ ì • ë°°ì¹˜ 10, ì¬ì‹œë„) =====
def post_with_retries(session, url, batch, headers):
    # payload í¬ê¸°(ë””ë²„ê¹…)
    try:
        payload = json.dumps(batch, ensure_ascii=False)
    except Exception as e:
        print(f"  â†³ JSON ì§ë ¬í™” ì‹¤íŒ¨: {e}")
        return False, None

    size_kb = len(payload.encode("utf-8")) / 1024.0
    for attempt in range(1, MAX_RETRIES_PER_BATCH + 1):
        try:
            resp = session.post(
                url,
                data=payload,  # json= ëŒ€ì‹  data= + ëª…ì‹œì  í—¤ë”
                headers=headers,
                timeout=(CONNECT_TIMEOUT, READ_TIMEOUT),
            )
            code = resp.status_code
            if 200 <= code < 300:
                return True, resp

            body_preview = (resp.text or "")[:200]
            print(f"  â†³ HTTP {code} (try {attempt}) | body: {body_preview!r}")

            if code in (408, 429, 500, 502, 503, 504):
                time.sleep(2 ** attempt)
                continue
            return False, resp

        except requests.exceptions.Timeout as e:
            print(f"  â†³ Timeout (try {attempt}) | batchâ‰ˆ{size_kb:.1f}KB | {e}")
            time.sleep(2 ** attempt)
        except requests.exceptions.RequestException as e:
            print(f"  â†³ RequestException (try {attempt}) | batchâ‰ˆ{size_kb:.1f}KB | {type(e).__name__}: {e}")
            time.sleep(2 ** attempt)

    return False, None

def send_in_batches(data, url, token):
    s = requests.Session()
    retry = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[500, 502, 503, 504],
        allowed_methods=["POST"],
        raise_on_status=False,
    )
    adapter = HTTPAdapter(max_retries=retry)
    s.mount("http://", adapter)
    s.mount("https://", adapter)

    headers = {
        "Authorization": f"Bearer {token}",
        "Accept": "application/json",
        "Content-Type": "application/json; charset=utf-8",
        "Connection": "keep-alive",
    }

    n = len(data)
    ok = 0
    fail = 0

    for st in range(0, n, BATCH_SIZE):
        ed = min(st + BATCH_SIZE, n)
        batch = data[st:ed]
        print(f"â†’ ì „ì†¡: {st+1}-{ed}/{n} (batch={len(batch)})")

        success, resp = post_with_retries(s, url, batch, headers)
        if success:
            ok += len(batch)
            print(f"  âœ… OK {len(batch)}ê±´ | ëˆ„ì ={ok}/{n}")
        else:
            msg = (resp.text[:200] if resp is not None else "no response")
            fail += len(batch)
            print(f"  âŒ FAIL {len(batch)}ê±´ | {msg!r}")
            # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰. ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨í•˜ë ¤ë©´ ë‹¤ìŒ ì¤„ì˜ ì£¼ì„ í•´ì œ:
            # break

    print(f"[ìš”ì•½] ì´ {n}ê±´ ì¤‘ {ok}ê±´ ì„±ê³µ, {fail}ê±´ ì‹¤íŒ¨")

# ===== ì‹¤í–‰ =====
total_data, seen_keys = [], set()
stats = {"sticky": 0, "normal": 0, "cutoff_stop": 0}

def make_dedupe_key(detail_url: str, ntt_sn: str | None) -> str:
    return f"sn:{ntt_sn}" if ntt_sn else f"url:{normalize_url(detail_url)}"

try:
    page, stop = 1, False
    while not stop:
        driver.get(f"{LIST_BASE}&currPage={page}")
        try:
            wait_for_list()
            items = scan_list_page()
            print(f"[ë””ë²„ê·¸] í˜ì´ì§€ {page} í•­ëª© ìˆ˜: {len(items)}")
        except TimeoutException:
            with open(f"list_{page}.html", "w", encoding="utf-8") as f:
                f.write(driver.page_source)
            print(f"âŒ ëª©ë¡ ë¡œë”© íƒ€ì„ì•„ì›ƒ (list_{page}.html ì €ì¥)")
            break

        if not items:
            break

        # ê³µì§€/ì¼ë°˜ í†µí•© ì²˜ë¦¬ (ë™ì¼ ê·œì¹™)
        page_cut = False
        for it in items:
            rd = it["row_date"]  # ëª©ë¡ì—ì„œ ë½‘íŒ ë‚ ì§œ (ì—†ì„ ìˆ˜ë„ ìˆìŒ)

            # í˜ì´ì§€ ì¤‘ì§€: 'ì¼ë°˜ê¸€'ì—ì„œë§Œ ì»¤íŠ¸ë¼ì¸ ë¯¸ë§Œì„ ë§Œë‚˜ë©´ ì¤‘ì§€
            # (ê³ ì • ê³µì§€ ë•Œë¬¸ì— ìŠ¤ìº”ì´ ëŠê¸°ëŠ” ê±¸ ë°©ì§€)
            if (not it["is_sticky"]) and rd and rd < CUTOFF_DATE:
                stats["cutoff_stop"] += 1
                page_cut = True
                break

            # 1ì°¨ í•„í„°: ëª©ë¡ ë‚ ì§œê°€ ì»¤íŠ¸ë¼ì¸ ë¯¸ë§Œì´ë©´ ìŠ¤í‚µ
            if rd and rd < CUTOFF_DATE:
                continue

            key = make_dedupe_key(it["detail_url"], it["ntt_sn"])
            if key in seen_keys:
                continue

            # ìƒì„¸ ì§„ì… â†’ ìµœì¢… ë‚ ì§œ í™•ì •(coalesce_date)
            content_html, thumb = fetch_detail_with_retry(it["detail_url"])
            final_date = datetime.fromisoformat(coalesce_date(rd, content_html)).date()
            if final_date < CUTOFF_DATE:
                continue

            payload = make_payload(it, content_html, thumb, it["is_sticky"])
            total_data.append(payload)
            seen_keys.add(key)
            stats["sticky" if it["is_sticky"] else "normal"] += 1

        if page_cut:
            stop = True
        else:
            page += 1

    print(f"[í†µê³„] ê³µì§€={stats['sticky']}, ì¼ë°˜={stats['normal']}, ì»¤íŠ¸ë¼ì¸ì¤‘ì§€={stats['cutoff_stop']}")
    print(f"[ì´ ì „ì†¡ ì˜ˆì •] {len(total_data)}ê±´")

    if total_data:
        send_in_batches(total_data, POST_ENDPOINT, ACCESS_TOKEN)
    else:
        print(f"â„¹ï¸ ì»¤íŠ¸ë¼ì¸({CUTOFF_DATE_STR}) ì´í›„ ì‹ ê·œ ê¸€ ì—†ìŒ ë˜ëŠ” ì¤‘ë³µ")

except Exception as e:
    print("âŒ ì „ì²´ ì˜¤ë¥˜ ë°œìƒ:", e)
finally:
    driver.quit()






